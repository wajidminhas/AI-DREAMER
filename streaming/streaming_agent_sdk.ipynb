{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+r6vwV7XsR9uOZkvUSFIb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajidminhas/AI-DREAMER/blob/main/streaming/streaming_agent_sdk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2YJW8YQ2UVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607b8d18-ffed-4113-8532-492d78e97939"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/120.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "FCvI5Iez3F4J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, OpenAIChatCompletionsModel, set_default_openai_client, set_default_openai_api, set_tracing_disabled, AsyncOpenAI\n",
        "from google.colab import userdata\n",
        "from agents.run import RunConfig\n",
        "import os\n",
        "\n",
        "set_tracing_disabled(True)\n",
        "\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "if not gemini_api_key:\n",
        "  raise ValueError(\"GEMINI_API_KEY not set\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key = gemini_api_key,\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    openai_client = external_client,\n",
        "    model= \"gemini-2.0-flash\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "gHyAUHYd3NMe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#now creating streaming event in agents sdk"
      ],
      "metadata": {
        "id": "tHT-WnIP7qve"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YTNG6QmaGQHY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "# from openai.types.responses import ResponseTextDeltaEvent\n",
        "# from agents import Agent, Runner\n",
        "\n",
        "async def joke_and_fun():\n",
        "    agent : Agent = Agent(\n",
        "        name= \"Joke & Fun\",\n",
        "        instructions=\"You are a Helplful Assitent and a Joke & fun actor\",\n",
        "        model= model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input= \"give five joke as fun\")\n",
        "    print(type(result), result ,\"\\n\\n\")\n",
        "    async for event in result.stream_events():\n",
        "        print(event)\n",
        "\n",
        "asyncio.run(joke_and_fun())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uQJ-SqDs7ac5",
        "outputId": "0dc23580-8384-467d-e6a5-a788cc4f0c4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'agents.result.RunResultStreaming'> RunResultStreaming:\n",
            "- Current agent: Agent(name=\"Joke & Fun\", ...)\n",
            "- Current turn: 0\n",
            "- Max turns: 10\n",
            "- Is complete: False\n",
            "- Final output (NoneType):\n",
            "    None\n",
            "- 0 new item(s)\n",
            "- 0 raw response(s)\n",
            "- 0 input guardrail result(s)\n",
            "- 0 output guardrail result(s)\n",
            "(See `RunResultStreaming` for more details) \n",
            "\n",
            "\n",
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joke & Fun', instructions='You are a Helplful Assitent and a Joke & fun actor', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7dfbdba07b90>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1748104176.272295, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Alright', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=', buckle up', item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" buttercup, because here come five jokes so funny they'll make your funny\", item_id='__fake_id__', output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" bone do the cha-cha!\\n\\n1.  Why don't scientists\", item_id='__fake_id__', output_index=0, sequence_number=6, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" trust atoms?\\n    *   Because they make up everything! (I know, I know, I'm a comedian, not a philosopher... mostly!)\\n\\n2.\", item_id='__fake_id__', output_index=0, sequence_number=7, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='  Why did the scarecrow win an award?\\n    *   Because he was outstanding in his field! (Get it?  *Waggles eyebrows', item_id='__fake_id__', output_index=0, sequence_number=8, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"*  I crack myself up!)\\n\\n3.  Parallel lines have so much in common.\\n    *   It's a shame they'll never meet. (Oh, the tragedy!  Quick, someone write a song!)\\n\\n4.\", item_id='__fake_id__', output_index=0, sequence_number=9, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='  Why did the bicycle fall over?\\n    *   Because it was two tired! (Seriously, I need a nap after telling that one.  Exhausting!)\\n\\n5.  What do you call a fish with no eyes?\\n', item_id='__fake_id__', output_index=0, sequence_number=10, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"    *   Fsh! (Okay, I admit, this one's terrible.  But I'm committed to the bit now!  *Bows dramatically*)\\n\\nHope those brought a smile to your face! If not, blame my agent. He booked me for this gig!\\n\", item_id='__fake_id__', output_index=0, sequence_number=11, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"Alright, buckle up buttercup, because here come five jokes so funny they'll make your funny bone do the cha-cha!\\n\\n1.  Why don't scientists trust atoms?\\n    *   Because they make up everything! (I know, I know, I'm a comedian, not a philosopher... mostly!)\\n\\n2.  Why did the scarecrow win an award?\\n    *   Because he was outstanding in his field! (Get it?  *Waggles eyebrows*  I crack myself up!)\\n\\n3.  Parallel lines have so much in common.\\n    *   It's a shame they'll never meet. (Oh, the tragedy!  Quick, someone write a song!)\\n\\n4.  Why did the bicycle fall over?\\n    *   Because it was two tired! (Seriously, I need a nap after telling that one.  Exhausting!)\\n\\n5.  What do you call a fish with no eyes?\\n    *   Fsh! (Okay, I admit, this one's terrible.  But I'm committed to the bit now!  *Bows dramatically*)\\n\\nHope those brought a smile to your face! If not, blame my agent. He booked me for this gig!\\n\", type='output_text'), sequence_number=12, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, buckle up buttercup, because here come five jokes so funny they'll make your funny bone do the cha-cha!\\n\\n1.  Why don't scientists trust atoms?\\n    *   Because they make up everything! (I know, I know, I'm a comedian, not a philosopher... mostly!)\\n\\n2.  Why did the scarecrow win an award?\\n    *   Because he was outstanding in his field! (Get it?  *Waggles eyebrows*  I crack myself up!)\\n\\n3.  Parallel lines have so much in common.\\n    *   It's a shame they'll never meet. (Oh, the tragedy!  Quick, someone write a song!)\\n\\n4.  Why did the bicycle fall over?\\n    *   Because it was two tired! (Seriously, I need a nap after telling that one.  Exhausting!)\\n\\n5.  What do you call a fish with no eyes?\\n    *   Fsh! (Okay, I admit, this one's terrible.  But I'm committed to the bit now!  *Bows dramatically*)\\n\\nHope those brought a smile to your face! If not, blame my agent. He booked me for this gig!\\n\", type='output_text')], role='assistant', status='completed', type='message'), output_index=0, sequence_number=13, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1748104176.272295, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, buckle up buttercup, because here come five jokes so funny they'll make your funny bone do the cha-cha!\\n\\n1.  Why don't scientists trust atoms?\\n    *   Because they make up everything! (I know, I know, I'm a comedian, not a philosopher... mostly!)\\n\\n2.  Why did the scarecrow win an award?\\n    *   Because he was outstanding in his field! (Get it?  *Waggles eyebrows*  I crack myself up!)\\n\\n3.  Parallel lines have so much in common.\\n    *   It's a shame they'll never meet. (Oh, the tragedy!  Quick, someone write a song!)\\n\\n4.  Why did the bicycle fall over?\\n    *   Because it was two tired! (Seriously, I need a nap after telling that one.  Exhausting!)\\n\\n5.  What do you call a fish with no eyes?\\n    *   Fsh! (Okay, I admit, this one's terrible.  But I'm committed to the bit now!  *Bows dramatically*)\\n\\nHope those brought a smile to your face! If not, blame my agent. He booked me for this gig!\\n\", type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=14, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joke & Fun', instructions='You are a Helplful Assitent and a Joke & fun actor', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7dfbdba07b90>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, buckle up buttercup, because here come five jokes so funny they'll make your funny bone do the cha-cha!\\n\\n1.  Why don't scientists trust atoms?\\n    *   Because they make up everything! (I know, I know, I'm a comedian, not a philosopher... mostly!)\\n\\n2.  Why did the scarecrow win an award?\\n    *   Because he was outstanding in his field! (Get it?  *Waggles eyebrows*  I crack myself up!)\\n\\n3.  Parallel lines have so much in common.\\n    *   It's a shame they'll never meet. (Oh, the tragedy!  Quick, someone write a song!)\\n\\n4.  Why did the bicycle fall over?\\n    *   Because it was two tired! (Seriously, I need a nap after telling that one.  Exhausting!)\\n\\n5.  What do you call a fish with no eyes?\\n    *   Fsh! (Okay, I admit, this one's terrible.  But I'm committed to the bit now!  *Bows dramatically*)\\n\\nHope those brought a smile to your face! If not, blame my agent. He booked me for this gig!\\n\", type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#this will output the text generated by the LLM token-by-token."
      ],
      "metadata": {
        "id": "0oP488N2IQau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joker\",\n",
        "        instructions=\"You are a helpful assistant.\",\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input=\"Please tell me 5 jokes.\")\n",
        "    async for event in result.stream_events():\n",
        "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
        "            print(event.data.delta, end=\"\", flush=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lm_npyd-piI",
        "outputId": "25762d8e-e270-46ea-bf00-f668fddd142e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Alright, here are 5 jokes for you:\n",
            "\n",
            "1.  Why don't scientists trust atoms?\n",
            "    Because they make up everything!\n",
            "\n",
            "2.  Why did the scarecrow win an award?\n",
            "    Because he was outstanding in his field!\n",
            "\n",
            "3.  What do you call a lazy kangaroo?\n",
            "    Pouch potato!\n",
            "\n",
            "4.  Why did the bicycle fall over?\n",
            "    Because it was two tired!\n",
            "\n",
            "5.  Parallel lines have so much in common.\n",
            "    It's a shame they'll never meet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#this will ignore raw events and stream updates to the user."
      ],
      "metadata": {
        "id": "ATTwM9HwI_5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import random\n",
        "from agents import Agent, ItemHelpers, Runner, function_tool\n",
        "\n",
        "@function_tool\n",
        "def how_many_jokes() -> int:\n",
        "    return random.randint(1, 10)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    agent = Agent(\n",
        "        name=\"Joke & Fun\",\n",
        "        instructions=\"First call the `how_many_jokes` tool, then tell that many jokes.\",\n",
        "        tools=[how_many_jokes],\n",
        "        model=model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(\n",
        "        agent,\n",
        "        input=\"Hello\",\n",
        "    )\n",
        "    print(\"=== Run starting ===\")\n",
        "\n",
        "    async for event in result.stream_events():\n",
        "        # We'll ignore the raw responses event deltas\n",
        "        if event.type == \"raw_response_event\":\n",
        "            continue\n",
        "        # When the agent updates, print that\n",
        "        elif event.type == \"agent_updated_stream_event\":\n",
        "            print(f\"Agent updated: {event.new_agent.name}\")\n",
        "            continue\n",
        "        # When items are generated, print them\n",
        "        elif event.type == \"run_item_stream_event\":\n",
        "            if event.item.type == \"tool_call_item\":\n",
        "                print(\"-- Tool was called\")\n",
        "            elif event.item.type == \"tool_call_output_item\":\n",
        "                print(f\"-- Tool output: {event.item.output}\")\n",
        "            elif event.item.type == \"message_output_item\":\n",
        "                print(f\"-- Message output:\\n {ItemHelpers.text_message_output(event.item)}\")\n",
        "            else:\n",
        "                pass  # Ignore other event types\n",
        "\n",
        "    print(\"=== Run complete ===\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_x-tphZI8Bn",
        "outputId": "36f901b2-5837-4e23-a835-7ea974364f0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Run starting ===\n",
            "Agent updated: Joke & Fun\n",
            "-- Tool was called\n",
            "-- Tool output: 7\n",
            "-- Message output:\n",
            " Ok, I will tell you 7 jokes.\n",
            "\n",
            "Why don’t scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n",
            "\n",
            "Why did the scarecrow win an award?\n",
            "\n",
            "Because he was outstanding in his field!\n",
            "\n",
            "What do you call a lazy kangaroo?\n",
            "\n",
            "Pouch potato!\n",
            "\n",
            "Why did the bicycle fall over?\n",
            "\n",
            "Because it was two tired!\n",
            "\n",
            "Why did the tomato turn red?\n",
            "\n",
            "Because it saw the salad dressing!\n",
            "\n",
            "What do you call a bear with no teeth?\n",
            "\n",
            "A gummy bear!\n",
            "\n",
            "Why did the teddy bear say no to dessert?\n",
            "\n",
            "Because she was stuffed!\n",
            "\n",
            "=== Run complete ===\n"
          ]
        }
      ]
    }
  ]
}