{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZkNxIlsg16vw13wlrydRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajidminhas/AI-DREAMER/blob/main/streaming_agent_sdk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "R2YJW8YQ2UVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b185749d-a5a3-46d0-b3cc-a21379cfa566"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/120.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.2/120.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/129.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m720.4/720.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai-agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "FCvI5Iez3F4J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Agent, Runner, OpenAIChatCompletionsModel, set_default_openai_client, set_default_openai_api, set_tracing_disabled, AsyncOpenAI\n",
        "from google.colab import userdata\n",
        "from agents.run import RunConfig\n",
        "import os\n",
        "\n",
        "set_tracing_disabled(True)\n",
        "\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "if not gemini_api_key:\n",
        "  raise ValueError(\"GEMINI_API_KEY not set\")\n",
        "\n",
        "external_client = AsyncOpenAI(\n",
        "    api_key = gemini_api_key,\n",
        "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "model = OpenAIChatCompletionsModel(\n",
        "    openai_client = external_client,\n",
        "    model= \"gemini-2.0-flash\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "gHyAUHYd3NMe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#now creating streaming event in agents sdk"
      ],
      "metadata": {
        "id": "tHT-WnIP7qve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "\n",
        "from openai.types.responses import ResponseTextDeltaEvent\n",
        "from agents import Agent, Runner\n",
        "\n",
        "async def joke_and_fun():\n",
        "    agent : Agent = Agent(\n",
        "        name= \"Joke & Fun\",\n",
        "        instructions=\"You are a Helplful Assitent and a Joke & fun actor\",\n",
        "        model= model\n",
        "    )\n",
        "\n",
        "    result = Runner.run_streamed(agent, input= \"give five joke as fun\")\n",
        "    # print(type(result), result ,\"\\n\\n\")\n",
        "    async for event in result.stream_events():\n",
        "        print(event)\n",
        "\n",
        "asyncio.run(joke_and_fun())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uQJ-SqDs7ac5",
        "outputId": "4660cfa8-4ea4-41dc-ef08-45773bbafb87"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AgentUpdatedStreamEvent(new_agent=Agent(name='Joke & Fun', instructions='You are a Helplful Assitent and a Joke & fun actor', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7a9f3444d510>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
            "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1747971628.4864826, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text'), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='Alright', item_id='__fake_id__', output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=', buckle up, buttercup, because here comes the comedy express! Choo cho', item_id='__fake_id__', output_index=0, sequence_number=4, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"o!\\n\\n1.  Why don't scientists trust atoms?\\n    \\\\\", item_id='__fake_id__', output_index=0, sequence_number=5, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"\\n    Because they make up everything! (I know, I know, it's atomic humor. Get it? *wink*)\\n\\n2.  Parallel\", item_id='__fake_id__', output_index=0, sequence_number=6, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" lines have so much in common.\\n    \\\\\\n    It's a shame they'll never meet. (Cue the sad trombone... *womp\", item_id='__fake_id__', output_index=0, sequence_number=7, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=' womp*)\\n\\n3.  Why did the scarecrow win an award?\\n    \\\\\\n    Because he was outstanding in his field! (He really knew how to bring the *straw* power!)\\n\\n4.  What do you', item_id='__fake_id__', output_index=0, sequence_number=8, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\" call a lazy kangaroo?\\n    \\\\\\n    Pouch potato! (Seriously though, those pouches look comfy. I'm jealous.)\\n\\n5.  I just saw a bank robbery on TV.\\n    \\\\\\n    It was un\", item_id='__fake_id__', output_index=0, sequence_number=9, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta=\"-bill-ievable! (I'm here all week! Try the veal...just kidding!)\\n\\nHope those brought a smile to your face! I'm ready with more if you need 'em! Just say the word! 😉\\n\", item_id='__fake_id__', output_index=0, sequence_number=10, type='response.output_text.delta'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text=\"Alright, buckle up, buttercup, because here comes the comedy express! Choo choo!\\n\\n1.  Why don't scientists trust atoms?\\n    \\\\\\n    Because they make up everything! (I know, I know, it's atomic humor. Get it? *wink*)\\n\\n2.  Parallel lines have so much in common.\\n    \\\\\\n    It's a shame they'll never meet. (Cue the sad trombone... *womp womp*)\\n\\n3.  Why did the scarecrow win an award?\\n    \\\\\\n    Because he was outstanding in his field! (He really knew how to bring the *straw* power!)\\n\\n4.  What do you call a lazy kangaroo?\\n    \\\\\\n    Pouch potato! (Seriously though, those pouches look comfy. I'm jealous.)\\n\\n5.  I just saw a bank robbery on TV.\\n    \\\\\\n    It was un-bill-ievable! (I'm here all week! Try the veal...just kidding!)\\n\\nHope those brought a smile to your face! I'm ready with more if you need 'em! Just say the word! 😉\\n\", type='output_text'), sequence_number=11, type='response.content_part.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, buckle up, buttercup, because here comes the comedy express! Choo choo!\\n\\n1.  Why don't scientists trust atoms?\\n    \\\\\\n    Because they make up everything! (I know, I know, it's atomic humor. Get it? *wink*)\\n\\n2.  Parallel lines have so much in common.\\n    \\\\\\n    It's a shame they'll never meet. (Cue the sad trombone... *womp womp*)\\n\\n3.  Why did the scarecrow win an award?\\n    \\\\\\n    Because he was outstanding in his field! (He really knew how to bring the *straw* power!)\\n\\n4.  What do you call a lazy kangaroo?\\n    \\\\\\n    Pouch potato! (Seriously though, those pouches look comfy. I'm jealous.)\\n\\n5.  I just saw a bank robbery on TV.\\n    \\\\\\n    It was un-bill-ievable! (I'm here all week! Try the veal...just kidding!)\\n\\nHope those brought a smile to your face! I'm ready with more if you need 'em! Just say the word! 😉\\n\", type='output_text')], role='assistant', status='completed', type='message'), output_index=0, sequence_number=12, type='response.output_item.done'), type='raw_response_event')\n",
            "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1747971628.4864826, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.0-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, buckle up, buttercup, because here comes the comedy express! Choo choo!\\n\\n1.  Why don't scientists trust atoms?\\n    \\\\\\n    Because they make up everything! (I know, I know, it's atomic humor. Get it? *wink*)\\n\\n2.  Parallel lines have so much in common.\\n    \\\\\\n    It's a shame they'll never meet. (Cue the sad trombone... *womp womp*)\\n\\n3.  Why did the scarecrow win an award?\\n    \\\\\\n    Because he was outstanding in his field! (He really knew how to bring the *straw* power!)\\n\\n4.  What do you call a lazy kangaroo?\\n    \\\\\\n    Pouch potato! (Seriously though, those pouches look comfy. I'm jealous.)\\n\\n5.  I just saw a bank robbery on TV.\\n    \\\\\\n    It was un-bill-ievable! (I'm here all week! Try the veal...just kidding!)\\n\\nHope those brought a smile to your face! I'm ready with more if you need 'em! Just say the word! 😉\\n\", type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, previous_response_id=None, reasoning=None, service_tier=None, status=None, text=None, truncation=None, usage=None, user=None), sequence_number=13, type='response.completed'), type='raw_response_event')\n",
            "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='Joke & Fun', instructions='You are a Helplful Assitent and a Joke & fun actor', handoff_description=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x7a9f3444d510>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, extra_query=None, extra_body=None, extra_headers=None), tools=[], mcp_servers=[], mcp_config={}, input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text=\"Alright, buckle up, buttercup, because here comes the comedy express! Choo choo!\\n\\n1.  Why don't scientists trust atoms?\\n    \\\\\\n    Because they make up everything! (I know, I know, it's atomic humor. Get it? *wink*)\\n\\n2.  Parallel lines have so much in common.\\n    \\\\\\n    It's a shame they'll never meet. (Cue the sad trombone... *womp womp*)\\n\\n3.  Why did the scarecrow win an award?\\n    \\\\\\n    Because he was outstanding in his field! (He really knew how to bring the *straw* power!)\\n\\n4.  What do you call a lazy kangaroo?\\n    \\\\\\n    Pouch potato! (Seriously though, those pouches look comfy. I'm jealous.)\\n\\n5.  I just saw a bank robbery on TV.\\n    \\\\\\n    It was un-bill-ievable! (I'm here all week! Try the veal...just kidding!)\\n\\nHope those brought a smile to your face! I'm ready with more if you need 'em! Just say the word! 😉\\n\", type='output_text')], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9lm_npyd-piI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}